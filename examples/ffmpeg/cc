Sim! Existem alguns headers prontos e simples no GitHub que podem te ajudar a salvar imagens como PNG em C/C++, sem precisar lidar com toda a complexidade do FFmpeg. Aqui vão duas opções populares:


---

1. stb_image_write.h (de Sean Barrett)

Extremamente fácil de usar.

Arquivo único (.h) — basta incluir no seu projeto.

Escreve PNG, BMP, TGA e JPG.


Como usar com RGB (como um frame PPM):

#define STB_IMAGE_WRITE_IMPLEMENTATION
#include "stb_image_write.h"

// Suponha que você tenha um buffer RGB de tamanho width * height * 3
stbi_write_png("frame.png", width, height, 3, rgb_buffer, width * 3);


---

2. lodepng

Específico para PNG.

Mais controle sobre compressão, chunks, etc.

Usa apenas C (sem dependências).


Uso básico:

#include "lodepng.h"

// Suponha que você tenha um buffer RGB
lodepng_encode24_file("frame.png", rgb_buffer, width, height);


---

Qual escolher?

Quer simplicidade e rapidez? Use stb_image_write.h.

Quer controle mais fino sobre PNG? Use lodepng.


Se quiser, posso te mandar um exemplo completo com stb_image_write.h. Deseja isso?




Ótimo! Se você está salvando frames como PPM via C usando o FFmpeg, pode salvá-los como PNG diretamente, com algumas mudanças no seu código. Aqui está o caminho geral:


---

1. Troque o formato de saída no seu código C

Você provavelmente está usando o AVCodec e salvando um AVFrame em PPM com algo como:

// Salvando como PPM
fprintf(f, "P6\n%d %d\n255\n", width, height);
fwrite(frame->data[0], 1, width * height * 3, f);

Em vez disso, para PNG, você precisa encodar a imagem com o codec PNG da libavcodec.


---

2. Use o encoder PNG da FFmpeg (libavcodec)

Aqui está um resumo dos passos:

1. Inicialize o encoder PNG (AVCodec *codec = avcodec_find_encoder(AV_CODEC_ID_PNG);)


2. Configure o AVCodecContext com os parâmetros certos (largura, altura, pixel format).


3. Envie o AVFrame para o encoder usando avcodec_send_frame.


4. Receba o AVPacket codificado com avcodec_receive_packet.


5. Escreva o conteúdo do AVPacket em um arquivo .png.




---

Exemplo Simplificado:

AVCodec *codec = avcodec_find_encoder(AV_CODEC_ID_PNG);
AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
codec_ctx->bit_rate = 400000;
codec_ctx->width = frame->width;
codec_ctx->height = frame->height;
codec_ctx->pix_fmt = AV_PIX_FMT_RGB24;
codec_ctx->time_base = (AVRational){1, 25};

avcodec_open2(codec_ctx, codec, NULL);

AVPacket *pkt = av_packet_alloc();
avcodec_send_frame(codec_ctx, frame);
avcodec_receive_packet(codec_ctx, pkt);

// Salve como .png
FILE *png_file = fopen("frame.png", "wb");
fwrite(pkt->data, 1, pkt->size, png_file);
fclose(png_file);

av_packet_free(&pkt);
avcodec_free_context(&codec_ctx);


---

Se seu AVFrame não estiver em AV_PIX_FMT_RGB24, você precisará converter com o sws_scale.

Deseja um exemplo mais completo com esse código funcionando?







Claro! Aqui está um exemplo completo em C usando FFmpeg + stb_image_write.h para abrir um vídeo, decodificar o primeiro frame e salvá-lo como PNG:


---

Pré-requisitos:

FFmpeg instalado (linkado com avformat, avcodec, swscale, etc.)

Baixe stb_image_write.h e coloque no mesmo diretório do código.



---

Código C:

// Compile com: gcc -o save_frame save_frame.c -lavformat -lavcodec -lavutil -lswscale
#include <stdio.h>
#include <libavformat/avformat.h>
#include <libavcodec/avcodec.h>
#include <libswscale/swscale.h>
#include <libavutil/imgutils.h>

#define STB_IMAGE_WRITE_IMPLEMENTATION
#include "stb_image_write.h"

int main(int argc, char *argv[]) {
    if (argc < 2) {
        printf("Uso: %s <arquivo_de_video>\n", argv[0]);
        return -1;
    }

    const char *input_filename = argv[1];

    avformat_network_init();

    AVFormatContext *fmt_ctx = NULL;
    if (avformat_open_input(&fmt_ctx, input_filename, NULL, NULL) < 0) {
        fprintf(stderr, "Erro ao abrir o arquivo de vídeo.\n");
        return -1;
    }

    if (avformat_find_stream_info(fmt_ctx, NULL) < 0) {
        fprintf(stderr, "Não foi possível obter as informações do stream.\n");
        return -1;
    }

    AVCodec *decoder = NULL;
    int video_stream_index = av_find_best_stream(fmt_ctx, AVMEDIA_TYPE_VIDEO, -1, -1, &decoder, 0);
    if (video_stream_index < 0) {
        fprintf(stderr, "Stream de vídeo não encontrado.\n");
        return -1;
    }

    AVCodecContext *codec_ctx = avcodec_alloc_context3(decoder);
    avcodec_parameters_to_context(codec_ctx, fmt_ctx->streams[video_stream_index]->codecpar);
    if (avcodec_open2(codec_ctx, decoder, NULL) < 0) {
        fprintf(stderr, "Erro ao abrir o codec.\n");
        return -1;
    }

    AVFrame *frame = av_frame_alloc();
    AVFrame *rgb_frame = av_frame_alloc();
    AVPacket *pkt = av_packet_alloc();

    struct SwsContext *sws_ctx = sws_getContext(
        codec_ctx->width, codec_ctx->height, codec_ctx->pix_fmt,
        codec_ctx->width, codec_ctx->height, AV_PIX_FMT_RGB24,
        SWS_BILINEAR, NULL, NULL, NULL
    );

    int num_bytes = av_image_get_buffer_size(AV_PIX_FMT_RGB24, codec_ctx->width, codec_ctx->height, 1);
    uint8_t *buffer = av_malloc(num_bytes * sizeof(uint8_t));
    av_image_fill_arrays(rgb_frame->data, rgb_frame->linesize, buffer, AV_PIX_FMT_RGB24, codec_ctx->width, codec_ctx->height, 1);

    int got_frame = 0;
    while (av_read_frame(fmt_ctx, pkt) >= 0) {
        if (pkt->stream_index == video_stream_index) {
            if (avcodec_send_packet(codec_ctx, pkt) == 0 &&
                avcodec_receive_frame(codec_ctx, frame) == 0) {

                sws_scale(sws_ctx,
                    (const uint8_t * const *)frame->data,
                    frame->linesize, 0, codec_ctx->height,
                    rgb_frame->data, rgb_frame->linesize);

                // Salva como PNG usando stb
                stbi_write_png("frame.png", codec_ctx->width, codec_ctx->height, 3, rgb_frame->data[0], rgb_frame->linesize[0]);
                printf("Frame salvo como frame.png\n");
                got_frame = 1;
                break;
            }
        }
        av_packet_unref(pkt);
    }

    if (!got_frame) {
        fprintf(stderr, "Nenhum frame foi decodificado.\n");
    }

    // Libera memória
    av_packet_free(&pkt);
    av_frame_free(&frame);
    av_frame_free(&rgb_frame);
    av_free(buffer);
    sws_freeContext(sws_ctx);
    avcodec_free_context(&codec_ctx);
    avformat_close_input(&fmt_ctx);

    return 0;
}


---

Compilação:

gcc -o save_frame save_frame.c -lavformat -lavcodec -lavutil -lswscale


---

Esse programa salva apenas o primeiro frame do vídeo como PNG. Se quiser salvar múltiplos frames, posso adaptar para isso.

Quer a versão para múltiplos frames ou preferiria outra biblioteca?








Claro! O campo linesize em FFmpeg (e outras bibliotecas de processamento de imagem) pode ser um pouco confuso à primeira vista, mas aqui vai uma explicação direta:


---

O que é linesize?

linesize (ou stride) representa quantos bytes existem em cada linha (linha de varredura) da imagem na memória, incluindo qualquer padding extra que possa existir entre as linhas.


---

Por que não é igual a width * bytes_por_pixel?

Na teoria, uma imagem RGB de width = 640 pixels deveria ter 640 * 3 = 1920 bytes por linha. Na prática, o linesize pode ser maior do que isso por questões de alinhamento de memória (por exemplo, alinhamento em múltiplos de 32 ou 64 bytes para otimização de acesso).

Exemplo:

width = 640, formato RGB24 (3 bytes/pixel)

Esperado: 1920 bytes por linha

linesize real: pode ser 1920, ou 1984, 2048, etc.


Ou seja, você deve usar linesize para percorrer as linhas, e não calcular manualmente width * bytes_por_pixel, porque isso pode gerar acesso inválido à memória.


---

Uso prático:

Se você quiser acessar um pixel (x, y) da imagem RGB:

uint8_t *linha = frame->data[0] + y * frame->linesize[0];
uint8_t r = linha[x * 3 + 0];
uint8_t g = linha[x * 3 + 1];
uint8_t b = linha[x * 3 + 2];


---

Resumo:

linesize[n] = número de bytes por linha no plano n (0 para RGB ou Y em YUV).

Pode ser maior que width * bytes_por_pixel.

Sempre use linesize para iterar as linhas corretamente.


Quer que eu mostre um desenho visual disso para facilitar ainda mais?





Você está quase certo! Vamos esclarecer exatamente os campos principais que o FFmpeg fornece em um AVFrame que representam uma imagem:


---

Principais campos relacionados à imagem em AVFrame:

1. width – Largura da imagem (em pixels)


2. height – Altura da imagem (em pixels)


3. linesize[] – Array com o stride (número de bytes por linha) para cada plano da imagem


4. data[] – Ponteiros para os dados da imagem em cada plano (por exemplo, Y, U, V ou RGB)




---

Como eles se relacionam:

Se for uma imagem RGB24, então data[0] é o único plano, e cada pixel usa 3 bytes.

Se for uma imagem YUV420p (formato comum em vídeos), então:

data[0] = plano Y (luminosidade)

data[1] = plano U (crominância azul)

data[2] = plano V (crominância vermelha)

linesize[0], linesize[1], linesize[2] dizem quantos bytes tem cada linha em cada plano.




---

Exemplo com RGB:

for (int y = 0; y < frame->height; y++) {
    uint8_t *linha = frame->data[0] + y * frame->linesize[0];
    for (int x = 0; x < frame->width; x++) {
        uint8_t r = linha[x * 3 + 0];
        uint8_t g = linha[x * 3 + 1];
        uint8_t b = linha[x * 3 + 2];
    }
}


---

Se quiser, posso gerar um diagrama visual explicando data, linesize, width e height. Quer isso?



